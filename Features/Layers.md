# Layers

Neural network layers are a fundamental building block of neural networks.
A neural network is composed of multiple layers of artificial neurons, or "nodes," that are connected by edges.
Each layer receives input from the previous layer, processes the input using weights that are specific to that layer, and produces an output
that is passed on to the next layer. There are several types of layers that are commonly used in neural networks, including:

Input layers: These are the first layers in a neural network, and they receive the raw input data.

Hidden layers: These layers come after the input layers and before the output layers.
They are called "hidden" because they are not directly connected to the input or output data.
Instead, they process the data using weights that are specific to that layer.

Output layers: These are the final layers in a neural network, and they produce the output predictions or classifications.

Each layer in a neural network is made up of many individual neurons, which are connected to the neurons in the next layer via edges.
The weights of these edges determine how much influence each neuron has on the output of the next layer.
Neural networks are able to learn and make predictions by adjusting the weights of the edges between neurons.
